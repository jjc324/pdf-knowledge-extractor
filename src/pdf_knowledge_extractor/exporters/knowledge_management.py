"""
Knowledge Management System Exporters
Support for Obsidian, Notion, Roam Research, Logseq, and Dendron
"""

import logging
import json
import yaml
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime
import re
import uuid
import time

from .base import BaseExporter, TemplateExporter, ExportConfig, ExportResult

logger = logging.getLogger(__name__)


class ObsidianExporter(TemplateExporter):
    """Export to Obsidian vault with wikilinks, tags, and graph connections."""
    
    @property
    def supported_formats(self) -> List[str]:
        return ['obsidian']
    
    @property
    def file_extension(self) -> str:
        return '.md'
    
    @property
    def default_templates(self) -> Dict[str, str]:
        return {
            'document': """# {title}

**Source:** {filename}
**Analyzed:** {timestamp}
**Concepts:** {concept_count}

## Summary
{summary}

## Key Concepts
{concepts_section}

## Related Documents
{related_documents}

## Tags
{tags}

---
*Generated by PDF Knowledge Extractor v2.2*
""",
            'concept': "- **[[{concept_text}]]** ({importance:.2f}) - {context}",
            'related_doc': "- [[{doc_title}]] (similarity: {similarity:.2f})",
            'cluster_note': """# Cluster: {cluster_label}

## Documents in this cluster:
{cluster_documents}

## Main Topics:
{main_topics}

## Coherence Score: {coherence_score:.2f}

{cluster_description}
"""
        }
    
    def export(self, analysis_data: Dict[str, Any], documents: Dict[str, str]) -> ExportResult:
        """Export to Obsidian vault structure."""
        start_time = time.time()
        
        try:
            # Validate configuration
            errors = self.validate_config()
            if errors:
                return self.create_export_result(False, self.config.output_path, [], 0, errors=errors)
            
            # Prepare vault directory
            vault_path = self.config.output_path
            if vault_path.suffix == '.md':
                vault_path = vault_path.parent / vault_path.stem
            
            vault_path.mkdir(parents=True, exist_ok=True)
            
            exported_docs = []
            total_concepts = 0
            total_relationships = 0
            
            # Create individual document notes
            for doc_id, doc_text in documents.items():
                doc_analysis = analysis_data.get('individual_analyses', {}).get(doc_id, {})
                semantic_data = analysis_data.get('semantic_analysis', {})
                
                # Get document concepts
                doc_concepts = []
                if semantic_data and semantic_data.get('concepts'):
                    doc_concepts = [c for c in semantic_data['concepts'] 
                                  if doc_id in c.get('document_ids', [])]
                    doc_concepts = self.filter_concepts(doc_concepts, doc_id)
                
                # Get related documents
                related_docs = []
                if semantic_data and semantic_data.get('similarities'):
                    for sim in semantic_data['similarities']:
                        if sim.get('doc1_id') == doc_id or sim.get('doc2_id') == doc_id:
                            other_doc = sim.get('doc2_id') if sim.get('doc1_id') == doc_id else sim.get('doc1_id')
                            if sim.get('similarity_score', 0) >= self.config.min_similarity_score:
                                related_docs.append({
                                    'doc_id': other_doc,
                                    'similarity': sim.get('similarity_score', 0)
                                })
                
                # Generate Obsidian tags
                tags = self._generate_obsidian_tags(doc_concepts, doc_analysis)
                
                # Create note content
                note_content = self._create_document_note(
                    doc_id, doc_text, doc_concepts, related_docs, tags, doc_analysis
                )
                
                # Write document note
                safe_filename = self._sanitize_filename(doc_id)
                note_path = vault_path / f"{safe_filename}.md"
                with open(note_path, 'w', encoding='utf-8') as f:
                    f.write(note_content)
                
                exported_docs.append(doc_id)
                total_concepts += len(doc_concepts)
                total_relationships += len(related_docs)
            
            # Create cluster notes if enabled
            if self.config.include_clusters and semantic_data.get('clusters'):
                self._create_cluster_notes(vault_path, semantic_data['clusters'])
            
            # Create concept index
            if self.config.include_concepts and semantic_data.get('concepts'):
                self._create_concept_index(vault_path, semantic_data['concepts'])
            
            # Create vault configuration
            self._create_obsidian_config(vault_path)
            
            execution_time = time.time() - start_time
            
            return self.create_export_result(
                success=True,
                output_path=vault_path,
                exported_docs=exported_docs,
                execution_time=execution_time,
                exported_concepts=total_concepts,
                exported_relationships=total_relationships,
                stats={'vault_files': len(list(vault_path.glob('*.md')))}
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Obsidian export failed: {e}")
            return self.create_export_result(
                False, self.config.output_path, [], execution_time, errors=[str(e)]
            )
    
    def _create_document_note(self, doc_id: str, doc_text: str, concepts: List[Dict],
                            related_docs: List[Dict], tags: List[str], analysis: Dict) -> str:
        """Create Obsidian note content for a document."""
        
        # Build concepts section
        concepts_section = ""
        for concept in concepts:
            concept_line = self.render_template('concept', {
                'concept_text': concept.get('text', ''),
                'importance': concept.get('importance_score', 0),
                'context': concept.get('context_sentences', [''])[0][:100] + '...' if concept.get('context_sentences') else ''
            })
            concepts_section += concept_line + "\n"
        
        # Build related documents section
        related_section = ""
        for rel_doc in related_docs:
            doc_title = self._sanitize_filename(rel_doc['doc_id'])
            related_line = self.render_template('related_doc', {
                'doc_title': doc_title,
                'similarity': rel_doc['similarity']
            })
            related_section += related_line + "\n"
        
        # Generate summary
        summary = self._generate_summary(doc_text, analysis)
        
        # Create note
        return self.render_template('document', {
            'title': self._sanitize_filename(doc_id),
            'filename': doc_id,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'concept_count': len(concepts),
            'summary': summary,
            'concepts_section': concepts_section,
            'related_documents': related_section or "No related documents found.",
            'tags': " ".join(f"#{tag}" for tag in tags)
        })
    
    def _generate_obsidian_tags(self, concepts: List[Dict], analysis: Dict) -> List[str]:
        """Generate appropriate Obsidian tags."""
        tags = ['pdf-analysis']
        
        # Add topic-based tags
        topics = analysis.get('topics', [])
        for topic in topics[:3]:  # Limit to top 3 topics
            tag = re.sub(r'[^\w\-]', '', topic.get('topic', '')).lower()
            if tag and len(tag) > 2:
                tags.append(tag)
        
        # Add concept-based tags
        for concept in concepts[:5]:  # Top 5 concepts
            if concept.get('concept_type') == 'keyword':
                tag = re.sub(r'[^\w\-]', '', concept.get('text', '')).lower()
                if tag and len(tag) > 2:
                    tags.append(tag)
        
        return list(set(tags))  # Remove duplicates
    
    def _create_cluster_notes(self, vault_path: Path, clusters: List[Dict]):
        """Create notes for document clusters."""
        clusters_dir = vault_path / "Clusters"
        clusters_dir.mkdir(exist_ok=True)
        
        for cluster in clusters:
            cluster_docs = ""
            for doc_id in cluster.get('document_ids', []):
                safe_filename = self._sanitize_filename(doc_id)
                cluster_docs += f"- [[{safe_filename}]]\n"
            
            main_topics = ""
            for topic in cluster.get('main_topics', []):
                main_topics += f"- {topic}\n"
            
            cluster_content = self.render_template('cluster_note', {
                'cluster_label': cluster.get('cluster_label', 'Unnamed Cluster'),
                'cluster_documents': cluster_docs,
                'main_topics': main_topics,
                'coherence_score': cluster.get('coherence_score', 0),
                'cluster_description': f"This cluster contains {len(cluster.get('document_ids', []))} related documents."
            })
            
            cluster_filename = self._sanitize_filename(cluster.get('cluster_label', f"cluster_{cluster.get('cluster_id')}"))
            cluster_path = clusters_dir / f"{cluster_filename}.md"
            
            with open(cluster_path, 'w', encoding='utf-8') as f:
                f.write(cluster_content)
    
    def _create_concept_index(self, vault_path: Path, concepts: List[Dict]):
        """Create an index of all concepts."""
        index_content = "# Concept Index\n\n"
        
        # Group concepts by type
        concepts_by_type = {}
        for concept in concepts:
            concept_type = concept.get('concept_type', 'unknown')
            if concept_type not in concepts_by_type:
                concepts_by_type[concept_type] = []
            concepts_by_type[concept_type].append(concept)
        
        for concept_type, type_concepts in concepts_by_type.items():
            index_content += f"## {concept_type.title()}s\n\n"
            
            # Sort by importance
            type_concepts.sort(key=lambda x: x.get('importance_score', 0), reverse=True)
            
            for concept in type_concepts:
                index_content += f"- **{concept.get('text', '')}** ({concept.get('importance_score', 0):.2f})\n"
                if concept.get('document_ids'):
                    doc_links = [f"[[{self._sanitize_filename(doc_id)}]]" 
                               for doc_id in concept.get('document_ids', [])]
                    index_content += f"  - Found in: {', '.join(doc_links)}\n"
            index_content += "\n"
        
        index_path = vault_path / "Concept Index.md"
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)
    
    def _create_obsidian_config(self, vault_path: Path):
        """Create Obsidian vault configuration."""
        obsidian_dir = vault_path / ".obsidian"
        obsidian_dir.mkdir(exist_ok=True)
        
        # Create app.json
        app_config = {
            "legacyEditor": False,
            "livePreview": True,
            "showLineNumber": True,
            "spellcheck": True,
            "useMarkdownLinks": False
        }
        
        with open(obsidian_dir / "app.json", 'w') as f:
            json.dump(app_config, f, indent=2)
        
        # Create graph.json for better graph visualization
        graph_config = {
            "search": "",
            "showTags": True,
            "showAttachments": False,
            "hideUnresolved": False,
            "showOrphans": True,
            "showArrow": True,
            "textFadeMultiplier": 0,
            "nodeSizeMultiplier": 1,
            "lineSizeMultiplier": 1,
            "centerStrength": 0.5,
            "repelStrength": 10,
            "linkStrength": 1,
            "linkDistance": 250,
            "scale": 1
        }
        
        with open(obsidian_dir / "graph.json", 'w') as f:
            json.dump(graph_config, f, indent=2)
    
    def _sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for filesystem compatibility."""
        # Remove/replace problematic characters
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # Remove leading/trailing dots and spaces
        sanitized = sanitized.strip('. ')
        # Limit length
        return sanitized[:200]
    
    def _generate_summary(self, doc_text: str, analysis: Dict) -> str:
        """Generate a summary of the document."""
        sentences = doc_text.split('.')[:3]  # First 3 sentences
        summary = '. '.join(sentences).strip()
        if len(summary) > 500:
            summary = summary[:500] + "..."
        return summary


class NotionExporter(TemplateExporter):
    """Export to Notion database format with relations and properties."""
    
    @property
    def supported_formats(self) -> List[str]:
        return ['notion']
    
    @property
    def file_extension(self) -> str:
        return '.json'
    
    @property
    def default_templates(self) -> Dict[str, str]:
        return {
            'database_schema': """{
    "title": "PDF Knowledge Base",
    "properties": {
        "Title": {"type": "title"},
        "Status": {"type": "select", "options": ["Analyzed", "Processing", "Error"]},
        "Source File": {"type": "rich_text"},
        "Concepts": {"type": "multi_select"},
        "Related Documents": {"type": "relation"},
        "Word Count": {"type": "number"},
        "Importance Score": {"type": "number"},
        "Created": {"type": "created_time"},
        "Last Edited": {"type": "last_edited_time"},
        "Tags": {"type": "multi_select"}
    }
}""",
            'page_content': """{
    "object": "page",
    "properties": {
        "Title": {"title": [{"text": {"content": "{title}"}}]},
        "Status": {"select": {"name": "Analyzed"}},
        "Source File": {"rich_text": [{"text": {"content": "{filename}"}}]},
        "Concepts": {"multi_select": {concepts_list}},
        "Word Count": {"number": {word_count}},
        "Tags": {"multi_select": {tags_list}}
    },
    "children": {content_blocks}
}"""
        }
    
    def export(self, analysis_data: Dict[str, Any], documents: Dict[str, str]) -> ExportResult:
        """Export to Notion database JSON format."""
        start_time = time.time()
        
        try:
            errors = self.validate_config()
            if errors:
                return self.create_export_result(False, self.config.output_path, [], 0, errors=errors)
            
            output_path = self.prepare_output_path()
            
            # Create Notion database structure
            notion_data = {
                "database_schema": json.loads(self.render_template('database_schema', {})),
                "pages": [],
                "metadata": {
                    "export_timestamp": datetime.now().isoformat(),
                    "total_documents": len(documents),
                    "generator": "PDF Knowledge Extractor v2.2"
                }
            }
            
            exported_docs = []
            total_concepts = 0
            
            # Create pages for each document
            for doc_id, doc_text in documents.items():
                doc_analysis = analysis_data.get('individual_analyses', {}).get(doc_id, {})
                semantic_data = analysis_data.get('semantic_analysis', {})
                
                # Get concepts for this document
                doc_concepts = []
                if semantic_data and semantic_data.get('concepts'):
                    doc_concepts = [c for c in semantic_data['concepts'] 
                                  if doc_id in c.get('document_ids', [])]
                    doc_concepts = self.filter_concepts(doc_concepts, doc_id)
                
                # Create Notion page
                page_data = self._create_notion_page(doc_id, doc_text, doc_concepts, doc_analysis)
                notion_data["pages"].append(page_data)
                
                exported_docs.append(doc_id)
                total_concepts += len(doc_concepts)
            
            # Write Notion export
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(notion_data, f, indent=2, ensure_ascii=False)
            
            execution_time = time.time() - start_time
            
            return self.create_export_result(
                success=True,
                output_path=output_path,
                exported_docs=exported_docs,
                execution_time=execution_time,
                exported_concepts=total_concepts,
                stats={'pages_created': len(notion_data["pages"])}
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Notion export failed: {e}")
            return self.create_export_result(
                False, self.config.output_path, [], execution_time, errors=[str(e)]
            )
    
    def _create_notion_page(self, doc_id: str, doc_text: str, concepts: List[Dict], analysis: Dict) -> Dict:
        """Create a Notion page structure."""
        
        # Prepare concepts for multi-select
        concept_options = [{"name": concept.get('text', '')[:100]} for concept in concepts[:20]]
        
        # Create content blocks
        content_blocks = [
            {
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": [{"text": {"content": "Summary"}}]}
            },
            {
                "object": "block", 
                "type": "paragraph",
                "paragraph": {"rich_text": [{"text": {"content": self._generate_summary(doc_text, analysis)}}]}
            },
            {
                "object": "block",
                "type": "heading_2", 
                "heading_2": {"rich_text": [{"text": {"content": "Key Concepts"}}]}
            }
        ]
        
        # Add concept blocks
        for concept in concepts[:10]:
            content_blocks.append({
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {
                    "rich_text": [{"text": {"content": f"{concept.get('text', '')} (importance: {concept.get('importance_score', 0):.2f})"}}]
                }
            })
        
        return {
            "object": "page",
            "properties": {
                "Title": {"title": [{"text": {"content": doc_id}}]},
                "Status": {"select": {"name": "Analyzed"}},
                "Source File": {"rich_text": [{"text": {"content": doc_id}}]},
                "Concepts": {"multi_select": concept_options},
                "Word Count": {"number": analysis.get('word_count', 0)}
            },
            "children": content_blocks
        }
    
    def _generate_summary(self, doc_text: str, analysis: Dict) -> str:
        """Generate document summary."""
        sentences = doc_text.split('.')[:2]
        summary = '. '.join(sentences).strip()
        return summary[:300] + "..." if len(summary) > 300 else summary


class RoamResearchExporter(TemplateExporter):
    """Export to Roam Research with block references and bidirectional links."""
    
    @property
    def supported_formats(self) -> List[str]:
        return ['roam']
    
    @property
    def file_extension(self) -> str:
        return '.json'
    
    @property
    def default_templates(self) -> Dict[str, str]:
        return {
            'page_template': """{title}
    Created:: {timestamp}
    Source:: {filename}
    Type:: #[[PDF Analysis]]
    
    Summary::
        {summary}
    
    Concepts::
{concepts_section}
    
    Related::
{related_section}
""",
            'concept_block': "        {concept_text} #concept\n            Importance:: {importance}\n            Context:: {context}",
            'related_block': "        [[{related_title}]] (similarity: {similarity})"
        }
    
    def export(self, analysis_data: Dict[str, Any], documents: Dict[str, str]) -> ExportResult:
        """Export to Roam Research JSON format."""
        start_time = time.time()
        
        try:
            errors = self.validate_config()
            if errors:
                return self.create_export_result(False, self.config.output_path, [], 0, errors=errors)
            
            output_path = self.prepare_output_path()
            
            # Create Roam export structure
            roam_data = {
                "title": "PDF Knowledge Base Export",
                "export_date": datetime.now().isoformat(),
                "pages": []
            }
            
            exported_docs = []
            total_concepts = 0
            
            # Create pages for each document
            for doc_id, doc_text in documents.items():
                doc_analysis = analysis_data.get('individual_analyses', {}).get(doc_id, {})
                semantic_data = analysis_data.get('semantic_analysis', {})
                
                page_data = self._create_roam_page(doc_id, doc_text, doc_analysis, semantic_data)
                roam_data["pages"].append(page_data)
                
                exported_docs.append(doc_id)
                total_concepts += len(page_data.get('concepts', []))
            
            # Add concept index page
            if self.config.include_concepts and analysis_data.get('semantic_analysis', {}).get('concepts'):
                concept_index = self._create_concept_index_page(analysis_data['semantic_analysis']['concepts'])
                roam_data["pages"].append(concept_index)
            
            # Write Roam export
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(roam_data, f, indent=2, ensure_ascii=False)
            
            execution_time = time.time() - start_time
            
            return self.create_export_result(
                success=True,
                output_path=output_path,
                exported_docs=exported_docs,
                execution_time=execution_time,
                exported_concepts=total_concepts,
                stats={'roam_pages': len(roam_data["pages"])}
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Roam Research export failed: {e}")
            return self.create_export_result(
                False, self.config.output_path, [], execution_time, errors=[str(e)]
            )
    
    def _create_roam_page(self, doc_id: str, doc_text: str, doc_analysis: Dict, semantic_data: Dict) -> Dict:
        """Create a Roam Research page."""
        
        # Get document concepts
        doc_concepts = []
        if semantic_data and semantic_data.get('concepts'):
            doc_concepts = [c for c in semantic_data['concepts'] 
                          if doc_id in c.get('document_ids', [])]
            doc_concepts = self.filter_concepts(doc_concepts, doc_id)
        
        # Build concepts section
        concepts_section = ""
        for concept in doc_concepts:
            concepts_section += self.render_template('concept_block', {
                'concept_text': concept.get('text', ''),
                'importance': concept.get('importance_score', 0),
                'context': concept.get('context_sentences', [''])[0][:100] if concept.get('context_sentences') else ''
            }) + "\n"
        
        # Build related documents section  
        related_section = ""
        if semantic_data and semantic_data.get('similarities'):
            for sim in semantic_data['similarities']:
                if (sim.get('doc1_id') == doc_id or sim.get('doc2_id') == doc_id) and \
                   sim.get('similarity_score', 0) >= self.config.min_similarity_score:
                    other_doc = sim.get('doc2_id') if sim.get('doc1_id') == doc_id else sim.get('doc1_id')
                    related_section += self.render_template('related_block', {
                        'related_title': other_doc,
                        'similarity': sim.get('similarity_score', 0)
                    }) + "\n"
        
        # Create page content
        page_content = self.render_template('page_template', {
            'title': doc_id,
            'timestamp': datetime.now().strftime('%B %d, %Y'),
            'filename': doc_id,
            'summary': self._generate_summary(doc_text, doc_analysis),
            'concepts_section': concepts_section,
            'related_section': related_section or "        No related documents found."
        })
        
        return {
            "title": doc_id,
            "content": page_content,
            "concepts": doc_concepts,
            "last_edited": datetime.now().isoformat()
        }
    
    def _create_concept_index_page(self, concepts: List[Dict]) -> Dict:
        """Create a Roam page for concept index."""
        content = "Concept Index #[[PDF Analysis]]\n"
        content += f"    Generated:: {datetime.now().strftime('%B %d, %Y')}\n\n"
        
        # Group by concept type
        concepts_by_type = {}
        for concept in concepts:
            concept_type = concept.get('concept_type', 'unknown')
            if concept_type not in concepts_by_type:
                concepts_by_type[concept_type] = []
            concepts_by_type[concept_type].append(concept)
        
        for concept_type, type_concepts in concepts_by_type.items():
            content += f"    {concept_type.title()}s::\n"
            type_concepts.sort(key=lambda x: x.get('importance_score', 0), reverse=True)
            
            for concept in type_concepts[:20]:  # Top 20 per type
                content += f"        [[{concept.get('text', '')}]] ({concept.get('importance_score', 0):.2f})\n"
        
        return {
            "title": "Concept Index",
            "content": content,
            "concepts": [],
            "last_edited": datetime.now().isoformat()
        }
    
    def _generate_summary(self, doc_text: str, analysis: Dict) -> str:
        """Generate document summary."""
        sentences = doc_text.split('.')[:2]
        summary = '. '.join(sentences).strip()
        return summary[:200] + "..." if len(summary) > 200 else summary


class LogseqExporter(TemplateExporter):
    """Export to Logseq with block structure and page references."""
    
    @property
    def supported_formats(self) -> List[str]:
        return ['logseq']
    
    @property
    def file_extension(self) -> str:
        return '.md'
    
    @property
    def default_templates(self) -> Dict[str, str]:
        return {
            'page_template': """title:: {title}
source:: {filename}
type:: #pdf-analysis
created:: {timestamp}

- ## Summary
  - {summary}

- ## Key Concepts
{concepts_section}

- ## Related Documents  
{related_section}

- ## Metadata
  - Word Count:: {word_count}
  - Concept Count:: {concept_count}
  - Analysis Date:: {timestamp}
""",
            'concept_block': "\t- **{concept_text}** #concept\n\t  - importance:: {importance}\n\t  - context:: {context}",
            'related_block': "\t- [[{related_title}]] (similarity: {similarity})"
        }
    
    def export(self, analysis_data: Dict[str, Any], documents: Dict[str, str]) -> ExportResult:
        """Export to Logseq format."""
        start_time = time.time()
        
        try:
            errors = self.validate_config()
            if errors:
                return self.create_export_result(False, self.config.output_path, [], 0, errors=errors)
            
            # Prepare Logseq directory
            logseq_path = self.config.output_path
            if logseq_path.suffix == '.md':
                logseq_path = logseq_path.parent / logseq_path.stem
            
            logseq_path.mkdir(parents=True, exist_ok=True)
            pages_dir = logseq_path / "pages"
            pages_dir.mkdir(exist_ok=True)
            
            exported_docs = []
            total_concepts = 0
            
            # Create pages for each document
            for doc_id, doc_text in documents.items():
                doc_analysis = analysis_data.get('individual_analyses', {}).get(doc_id, {})
                semantic_data = analysis_data.get('semantic_analysis', {})
                
                page_content = self._create_logseq_page(doc_id, doc_text, doc_analysis, semantic_data)
                
                # Write page file
                safe_filename = self._sanitize_filename(doc_id)
                page_path = pages_dir / f"{safe_filename}.md"
                
                with open(page_path, 'w', encoding='utf-8') as f:
                    f.write(page_content)
                
                exported_docs.append(doc_id)
                # Count concepts in the content
                total_concepts += page_content.count('#concept')
            
            # Create Logseq config
            self._create_logseq_config(logseq_path)
            
            execution_time = time.time() - start_time
            
            return self.create_export_result(
                success=True,
                output_path=logseq_path,
                exported_docs=exported_docs,
                execution_time=execution_time,
                exported_concepts=total_concepts,
                stats={'logseq_pages': len(exported_docs)}
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Logseq export failed: {e}")
            return self.create_export_result(
                False, self.config.output_path, [], execution_time, errors=[str(e)]
            )
    
    def _create_logseq_page(self, doc_id: str, doc_text: str, doc_analysis: Dict, semantic_data: Dict) -> str:
        """Create Logseq page content."""
        
        # Get document concepts
        doc_concepts = []
        if semantic_data and semantic_data.get('concepts'):
            doc_concepts = [c for c in semantic_data['concepts'] 
                          if doc_id in c.get('document_ids', [])]
            doc_concepts = self.filter_concepts(doc_concepts, doc_id)
        
        # Build concepts section
        concepts_section = ""
        for concept in doc_concepts:
            concepts_section += self.render_template('concept_block', {
                'concept_text': concept.get('text', ''),
                'importance': concept.get('importance_score', 0),
                'context': concept.get('context_sentences', [''])[0][:100] if concept.get('context_sentences') else ''
            }) + "\n"
        
        # Build related documents section
        related_section = ""
        if semantic_data and semantic_data.get('similarities'):
            for sim in semantic_data['similarities']:
                if (sim.get('doc1_id') == doc_id or sim.get('doc2_id') == doc_id) and \
                   sim.get('similarity_score', 0) >= self.config.min_similarity_score:
                    other_doc = sim.get('doc2_id') if sim.get('doc1_id') == doc_id else sim.get('doc1_id')
                    related_section += self.render_template('related_block', {
                        'related_title': self._sanitize_filename(other_doc),
                        'similarity': sim.get('similarity_score', 0)
                    }) + "\n"
        
        return self.render_template('page_template', {
            'title': doc_id,
            'filename': doc_id,
            'timestamp': datetime.now().strftime('%Y-%m-%d'),
            'summary': self._generate_summary(doc_text, doc_analysis),
            'concepts_section': concepts_section,
            'related_section': related_section or "\t- No related documents found.",
            'word_count': doc_analysis.get('word_count', 0),
            'concept_count': len(doc_concepts)
        })
    
    def _create_logseq_config(self, logseq_path: Path):
        """Create Logseq configuration."""
        config_dir = logseq_path / "logseq"
        config_dir.mkdir(exist_ok=True)
        
        config = {
            "preferred-format": "markdown",
            "pages-directory": "pages",
            "journals-directory": "journals",
            "default-templates": {
                "journals": "journals.md"
            },
            "feature": {
                "enable-block-timestamps?": True,
                "enable-search-remove-accents?": True
            }
        }
        
        with open(config_dir / "config.edn", 'w') as f:
            f.write(str(config).replace("'", '"').replace("True", "true").replace("False", "false"))
    
    def _sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for Logseq."""
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
        sanitized = re.sub(r'[^\w\s\-_.]', '', sanitized)
        return sanitized.strip('. ')[:200]
    
    def _generate_summary(self, doc_text: str, analysis: Dict) -> str:
        """Generate document summary."""
        sentences = doc_text.split('.')[:2]
        summary = '. '.join(sentences).strip()
        return summary[:250] + "..." if len(summary) > 250 else summary


class DendronExporter(TemplateExporter):
    """Export to Dendron VSCode workspace with schemas and hierarchies."""
    
    @property
    def supported_formats(self) -> List[str]:
        return ['dendron']
    
    @property
    def file_extension(self) -> str:
        return '.md'
    
    @property
    def default_templates(self) -> Dict[str, str]:
        return {
            'document_template': """---
id: {note_id}
title: {title}
desc: PDF analysis for {filename}
updated: {timestamp}
created: {timestamp}
tags: [pdf-analysis, {tags}]
---

# {title}

## Summary
{summary}

## Key Concepts
{concepts_section}

## Related Documents
{related_section}

## Metadata
- Source: {filename}  
- Word Count: {word_count}
- Concepts: {concept_count}
- Analysis Date: {timestamp}
""",
            'concept_template': "- **{concept_text}** (importance: {importance:.2f})\n  - Context: {context}",
            'schema_template': """---
id: pdf-analysis
title: PDF Analysis Schema
desc: Schema for PDF document analysis
version: 1
imports: []
schemas:
  - id: document
    children:
      - concepts
      - metadata
      - relationships
    title: PDF Document
    namespace: true
  - id: concept
    title: Extracted Concept
    template:
      id: concepts.{{fname}}
      title: "{{fname}}"
      desc: "Concept extracted from PDF analysis"
      tags: [concept, pdf-analysis]
"""
        }
    
    def export(self, analysis_data: Dict[str, Any], documents: Dict[str, str]) -> ExportResult:
        """Export to Dendron workspace structure."""
        start_time = time.time()
        
        try:
            errors = self.validate_config()
            if errors:
                return self.create_export_result(False, self.config.output_path, [], 0, errors=errors)
            
            # Prepare Dendron workspace
            workspace_path = self.config.output_path
            if workspace_path.suffix == '.md':
                workspace_path = workspace_path.parent / workspace_path.stem
            
            workspace_path.mkdir(parents=True, exist_ok=True)
            
            exported_docs = []
            total_concepts = 0
            
            # Create schema file
            self._create_dendron_schema(workspace_path)
            
            # Create document notes
            for doc_id, doc_text in documents.items():
                doc_analysis = analysis_data.get('individual_analyses', {}).get(doc_id, {})
                semantic_data = analysis_data.get('semantic_analysis', {})
                
                note_content = self._create_dendron_note(doc_id, doc_text, doc_analysis, semantic_data)
                
                # Create hierarchical filename
                safe_filename = self._sanitize_filename(doc_id)
                note_filename = f"pdf-analysis.document.{safe_filename}.md"
                note_path = workspace_path / note_filename
                
                with open(note_path, 'w', encoding='utf-8') as f:
                    f.write(note_content)
                
                exported_docs.append(doc_id)
                total_concepts += note_content.count('- **')
            
            # Create workspace configuration
            self._create_dendron_config(workspace_path)
            
            execution_time = time.time() - start_time
            
            return self.create_export_result(
                success=True,
                output_path=workspace_path,
                exported_docs=exported_docs,
                execution_time=execution_time,
                exported_concepts=total_concepts,
                stats={'dendron_notes': len(exported_docs)}
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Dendron export failed: {e}")
            return self.create_export_result(
                False, self.config.output_path, [], execution_time, errors=[str(e)]
            )
    
    def _create_dendron_note(self, doc_id: str, doc_text: str, doc_analysis: Dict, semantic_data: Dict) -> str:
        """Create Dendron note content."""
        
        # Get document concepts
        doc_concepts = []
        if semantic_data and semantic_data.get('concepts'):
            doc_concepts = [c for c in semantic_data['concepts'] 
                          if doc_id in c.get('document_ids', [])]
            doc_concepts = self.filter_concepts(doc_concepts, doc_id)
        
        # Build concepts section
        concepts_section = ""
        for concept in doc_concepts:
            concepts_section += self.render_template('concept_template', {
                'concept_text': concept.get('text', ''),
                'importance': concept.get('importance_score', 0),
                'context': concept.get('context_sentences', [''])[0][:100] if concept.get('context_sentences') else ''
            }) + "\n"
        
        # Build related documents section
        related_section = ""
        if semantic_data and semantic_data.get('similarities'):
            for sim in semantic_data['similarities']:
                if (sim.get('doc1_id') == doc_id or sim.get('doc2_id') == doc_id) and \
                   sim.get('similarity_score', 0) >= self.config.min_similarity_score:
                    other_doc = sim.get('doc2_id') if sim.get('doc1_id') == doc_id else sim.get('doc1_id')
                    safe_other = self._sanitize_filename(other_doc)
                    related_section += f"- [[pdf-analysis.document.{safe_other}]] (similarity: {sim.get('similarity_score', 0):.2f})\n"
        
        # Generate tags from topics
        tags = []
        topics = doc_analysis.get('topics', [])
        for topic in topics[:3]:
            tag = re.sub(r'[^\w\-]', '', topic.get('topic', '')).lower()
            if tag and len(tag) > 2:
                tags.append(tag)
        
        return self.render_template('document_template', {
            'note_id': str(uuid.uuid4()),
            'title': doc_id,
            'filename': doc_id,
            'timestamp': int(time.time() * 1000),  # Dendron uses milliseconds
            'tags': ', '.join(tags),
            'summary': self._generate_summary(doc_text, doc_analysis),
            'concepts_section': concepts_section,
            'related_section': related_section or "- No related documents found.",
            'word_count': doc_analysis.get('word_count', 0),
            'concept_count': len(doc_concepts)
        })
    
    def _create_dendron_schema(self, workspace_path: Path):
        """Create Dendron schema file."""
        schema_content = self.render_template('schema_template', {})
        
        with open(workspace_path / "schema.yml", 'w', encoding='utf-8') as f:
            f.write(schema_content)
    
    def _create_dendron_config(self, workspace_path: Path):
        """Create Dendron workspace configuration."""
        dendron_config = {
            "version": 5,
            "dev": {
                "enablePreviewV2": True
            },
            "commands": {
                "lookup": {
                    "note": {
                        "selectionType": "extract"
                    }
                }
            },
            "workspace": {
                "vaults": [
                    {
                        "fsPath": ".",
                        "name": "pdf-analysis"
                    }
                ]
            },
            "site": {
                "copyAssets": True,
                "siteHierarchies": ["pdf-analysis"],
                "siteRootDir": "docs",
                "usePrettyRefs": True
            }
        }
        
        with open(workspace_path / "dendron.yml", 'w') as f:
            yaml.dump(dendron_config, f, default_flow_style=False)
    
    def _sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for Dendron hierarchies."""
        sanitized = re.sub(r'[<>:"/\\|?*\s]', '-', filename)
        sanitized = re.sub(r'[^\w\-.]', '', sanitized)
        sanitized = re.sub(r'-+', '-', sanitized)
        return sanitized.strip('.-')[:100]
    
    def _generate_summary(self, doc_text: str, analysis: Dict) -> str:
        """Generate document summary."""
        sentences = doc_text.split('.')[:2]
        summary = '. '.join(sentences).strip()
        return summary[:300] + "..." if len(summary) > 300 else summary